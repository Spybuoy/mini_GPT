# mini_GPT
Introduction
This project is a simplified version of the GPT. It consists of the following components:
Multi-Head Attention
FeedForward Layer
Transformer Blocks
Positional Encoding
Token Embedding

#Hyperparameters
The model is trained with the following hyperparameters:

Batch Size: 16
Block Size: 32
Max Iterations: 5000
Evaluation Interval: 100
Learning Rate: 1e-3
Device: "cuda" if available, else "cpu"
Evaluation Iterations: 200
Embedding Dimension: 64
Number of Heads: 4
Number of Layers: 4
Dropout: 0.0

#Data
The model is trained on the data provided in the input.txt file located in the repo.
